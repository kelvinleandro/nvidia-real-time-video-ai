{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddca83a8",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b113e10-1b6d-449b-b712-f79f3f610ef5",
   "metadata": {},
   "source": [
    "# Preparation for Model Training #\n",
    "We noticed some issues with the TrafficCamNet model for our video AI application. It's likely that the model wasn't trained for our exact parking garage use case. For the remaining of the lab, we will use the TAO Toolkit to fine-tune the model so that it can adapt to our environment. Below is what a typical model development workflow looks like. We start by preparing a pre-trained model and the data. Next, we prepare the configuration file(s) and begin to train the model with new data and evaluate its performance. We export the model once its satisfactory. Note that this does not include inference optimization steps, which is very important for video AI applications that are deployed on edge devices. \n",
    "<p><img src='images/pre-trained_model_workflow.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1934605-75a4-49fa-9a5d-214f4a001400",
   "metadata": {},
   "source": [
    "## Learning Objectives ##\n",
    "In this notebook, you will learn how to prepare for training a video AI model using the TAO Toolkit, including: \n",
    "* Understanding Model Specification\n",
    "* Preparing Data for TAO Toolkit Consumption\n",
    "* Editing Spec Files for TAO Toolkit Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756103b-e433-42e4-a9ec-9245d6a64b51",
   "metadata": {},
   "source": [
    "**Table of Contents**<br>\n",
    "This notebook covers the below sections: \n",
    "1. [Detectnet_v2 Object Detection Model](#s1)\n",
    "    * [Directory Structure](#s1.1)\n",
    "    * [Model Objective](#s1.2)\n",
    "2. [Prepare Pre-trained Model](#s2)\n",
    "    * [Exercise #1 - Review Model Card](#e1)\n",
    "3. [Prepare Data Set](#s3)\n",
    "    * [Annotation](#s3.1)\n",
    "    * [Exploratory Data Analysis](#s3.2)\n",
    "    * [Covert Video File into Frame Images](#s3.3)\n",
    "    * [Generate Labels](#s3.4)\n",
    "    * [Converting to TFRecords](#s3.5)\n",
    "    * [Exercise #2 Dataset Convert](#e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fcd16-3d29-405d-b032-6b48226dad63",
   "metadata": {},
   "source": [
    "<a name='s1'></a>\n",
    "## DetectNet_v2 Object Detection Model ##\n",
    "As we previously learned, the [TrafficCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/trafficcamnet) purpose-built model is based on NVIDIA DetectNet_v2 detector with ResNet18 as a feature extractor. As such, we will use the `detectnet_v2` task, which supports the following subtasks: \n",
    "* `dataset_convert`\n",
    "* `train`\n",
    "* `evaluate`\n",
    "* `inference`\n",
    "* `prune`\n",
    "* `calibration_tensorfile`\n",
    "* `export`\n",
    "\n",
    "<p><img src='images/rewind.png' width=720><p>\n",
    "    \n",
    "These subtasks can be invoked using the convention `detectnet_v2 <subtask> <args_per_subtask>` on the command-line. Additionally, we can always find more information about these subtasks with `detector_v2 <subtask> --help`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259d244-84ef-44ff-a389-d4a6eeab7571",
   "metadata": {},
   "source": [
    "<a name='s1.1'></a>\n",
    "### Directory Structure ###\n",
    "We will use the below structure for our project, where the `tao_project` directory will hold most of the assets related to model training and outputs. \n",
    "\n",
    "<p><img src='images/project_structure.png' width=740></p>\n",
    "\n",
    "* The current directory is `/dli/task`. When using paths, it is most reliable to use the absolute path that begins with `/dli/task` as some of the functions will otherwise try to reference the paths relative to where they are called. \n",
    "* The higher level `data` directory represents the raw video data, vs. the lower level `tao_project/data` directory represents the preprocessed data to be used for model training. \n",
    "* The higher level `images` directory contains graphics used in this course and are not related to the video AI model. \n",
    "* The `spec_files` directory holds spec files that will be used for TAO Toolkit model training as well as DeepStream `Gst-nvinfer` plugin configuration files. \n",
    "* The `tao_project/models` directory will hold different versions of the model as we work to arrive at a final, optimized, product. Each folder will hold the corresponding model file (e.g. `.tlt` or `.etlt`), as well as accompanied assets such as `labels.txt`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2370d9e-d605-46ab-b474-2db51ef29635",
   "metadata": {},
   "source": [
    "Execute the below cell to set and create directories for the TAO Toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b5c30-81b7-4a34-9789-b59614f44d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Set and create directories for the TAO Toolkit experiment\n",
    "import os\n",
    "\n",
    "os.environ['PROJECT_DIR']='/dli/task/tao_project'\n",
    "os.environ['SOURCE_DATA_DIR']='/dli/task/data'\n",
    "os.environ['DATA_DIR']='/dli/task/tao_project/data'\n",
    "os.environ['MODELS_DIR']='/dli/task/tao_project/models'\n",
    "os.environ['SPEC_FILES_DIR']='/dli/task/spec_files'\n",
    "\n",
    "!mkdir $PROJECT_DIR\n",
    "!mkdir $DATA_DIR\n",
    "!mkdir $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4977de-9671-40ec-8324-d05803074c43",
   "metadata": {},
   "source": [
    "<a name='s1.2'></a>\n",
    "### Model Objective ###\n",
    "For our video AI application, we want to train a model that uses the TrafficCamNet as the starting point and provide it with additional (labeled) data so it can adapt to our specific camera angle, lighting condition, and other environmental conditions. We will be using the **unpruned pre-trained TrafficCamNet purpose-built model** as the starting point and training a custom **one-class Object Detection model** that is adapted to our use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a15e30",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2'></a>\n",
    "## Prepare Pre-trained Model and Data Set ##\n",
    "Developers typically begin by choosing and downloading a pre-trained model from [NGC](https://ngc.nvidia.com/) - either a highly accurate purpose-built model or just the pre-trained weights of the architecture of their choice. It's difficult to immediately identify which model/architecture will work best for a specific use case as there is often a tradeoff between time to train, accuracy, and inference performance. It is common to compare across multiple models before picking the best candidate.\n",
    "\n",
    "Here are some pointers that will help choose an appropriate model: \n",
    "* Look at the model inputs/outputs to decide if it will fit your use case. \n",
    "* Input format is also an important consideration. For example, TrafficCamNet, as well as other DetectNet_v2 models, expect the input to be 0-1 normalized with input channels in RGB order. Models that use a BGR order will require input preprocessing/mean subtraction that might result in suboptimal performance. \n",
    "\n",
    "We can use the `ngc registry model list <model_glob_string>` command to get a list of models that are hosted in the NGC model registry. For example, we can use `ngc registry model list nvidia/tao/*` to list all available models. The `--column` option identifies the columns of interest. More information about the NGC Registry CLI can be found in the [User Guide](https://docs.nvidia.com/dgx/pdf/ngc-registry-cli-user-guide.pdf). For each model, there is a pruned version that can be deployed as is or an unpruned version which can be used to re-train with more data for specific use cases. We will use the unpruned version as a start for training purposes. The `ngc registry model download-version <org>/[<team>/]<model-name:version>` command will download the model from the registry. It has a `--dest` option to specify the path to download directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8730b58",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise #1 - Review Model Cards ####\n",
    "Let's download a pre-trained model. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Review the model cards for [TrafficCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/trafficcamnet) and/or [DetectNet_v2](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pretrained_detectnet_v2) models to understand where you can find important model specifications. \n",
    "* Execute the below cell to download the NGC CLI. \n",
    "* Execute the following cell to list all available models. \n",
    "* Execute the following cell to download the TrafficCamNet model. \n",
    "* Execute the following cell to check if the model has been downloaded. \n",
    "* Execute the following cell to create `labels.txt` if it doesn't exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88994507",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Download the NGC CLI\n",
    "%env CLI=ngccli_linux.zip\n",
    "!mkdir -p ngc_assets/ngccli\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P ngc_assets/ngccli\n",
    "!unzip -u \"ngc_assets/ngccli/$CLI\" \\\n",
    "       -d ngc_assets/ngccli/\n",
    "!rm ngc_assets/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(\"ngc_assets\", os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1394b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# List all available models\n",
    "!ngc registry model list nvidia/tao/* --column name --column repository --column application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab3f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Download the unpruned pre-trained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/trafficcamnet:unpruned_v1.0 \\\n",
    "    --dest $MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a7ac2-cf8e-4536-98a0-066dea725ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Download the pruned pre-trained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/trafficcamnet:pruned_v1.0 \\\n",
    "    --dest $MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03eafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Check if models have been downloaded into directory\n",
    "!ls -rlt $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb57ff2-2960-4dc2-80e0-7ebae65debc8",
   "metadata": {},
   "source": [
    "**Observations**:<br>\n",
    "Below are some fields that are important to note: \n",
    "\n",
    "<p><img src='images/model_card_tao.png' width=1080></p>\n",
    "\n",
    "<p><img src='images/encryption_key.png' width=540></p>\n",
    "\n",
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "_Note that we're using the purpose-built TrafficCamNet model as the starting point for scene adaptation. Feel free to try the lab with other model architectures if there is time left at the end of the course. When working with purpose-built models from NGC, the correct **encryption key** is required to load the model. Users will be able to define their own export encryption key when training from a general purpose model. This is to protect proprietary IP and used to decrypt the `.etlt` model in DeepStream applications._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a21c3",
   "metadata": {},
   "source": [
    "<a name='s3'></a>\n",
    "## Prepare Data Set ##\n",
    "We're going to use annotated video data shot from the same camera at the NVIDIA headquarters parking lot for our model. It's important to recognize that the data that we're providing is limited and insufficient for training a model from scratch. Using the TAO Toolkit and transfer learning, we can use the TrafficCamNet model as the starting point and train a custom model. This is a common case when we can leverage TAO Toolkit's scene/domain adaptation capabilities. \n",
    "\n",
    "The TAO Toolkit requires the data to be in a specific format for training and evaluation: \n",
    "* The object detection tasks in the TAO Toolkit expects data in the `KITTI format`. \n",
    "    * The `images` directory contains the images to train on. \n",
    "    * The `labels` directory contains labels to the corresponding images. \n",
    "    * The `kitti_seq_to_map.json` file is _optional_ and contains a sequence to frame ID mapping for the frames in the images directory. It is useful if the data needs to be split into folds sequence wise. \n",
    "\n",
    "<p><img src='images/detection_input.png' width=720></p>\n",
    "\n",
    "* For comparison, the classification tasks expects a directory of images with the following structure, where each class has its own directory with the class name. \n",
    "\n",
    "<p><img src='images/classification_input.png' width=720></p>\n",
    "\n",
    "_You can find more details on data annotation format in the [TAO Toolkit User Guide](https://docs.nvidia.com/metropolis/TLT/tlt-user-guide/text/data_annotation_format.html#object-detection-kitti-format)_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9dc917",
   "metadata": {},
   "source": [
    "<a name='s3.1'></a>\n",
    "### Annotation ###\n",
    "Let's preview the video before moving forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1e8b9-ccf9-4285-aa2f-46135f658087",
   "metadata": {},
   "source": [
    "Execute the below cell to view the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ad2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View the video\n",
    "from IPython.display import Video\n",
    "\n",
    "Video(\"data/126_206-A0-3_raw.mp4\", width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e79388",
   "metadata": {},
   "source": [
    "In addition to video feeds, we need labels to evaluate the inference results (compare the actual objects to those detected by our deep learning models), and also to expand the ground truth for training purposes. This is normally a time consuming process, but this requirement is significantly reduced with transfer learning. There are a number of annotation tools that are publically available for use. Our data set annotation was generated (manually) using [Vatic](https://github.com/cvondrick/vatic) and provided in JSON format. Each entry starts with the `track_id`, representing a unique index for each car within the recording. The `track_id` provides a set of bounding boxes and their respective bounding box positions. Below, you can see elements of the annotation format:\n",
    "\n",
    "<p><img src=\"images/vatic.jpg\" width=720></p>\n",
    "\n",
    "This is a snapshot of the JSON file for the video. We're mainly interested in the bounding box coordinates captured for our object detection model: \n",
    "* **xbr**: an integer in the range between [0, frame width], indicating the right-most location of the bounding box in coordinates relative to the frame size.<br />\n",
    "* **xtl**: an integer in the range between [0, frame width], indicating the left-most location of the bounding box in coordinates relative to the frame size.<br />\n",
    "* **ybr**: an integer in the range between [0, frame height], indicating the bottom-most location of the bounding box in coordinates relative to the frame size.<br />\n",
    "* **ytl**: an integer in the range between [0, frame height], indicating the top-most location of the bounding box in coordinates relative to the frame size.<br />\n",
    "<p><img src=\"images/json_structure.png\" width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20e908",
   "metadata": {},
   "source": [
    "Execute the cell below to preview the annotation in JSON format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42767e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Preview the annotation\n",
    "!cat $SOURCE_DATA_DIR/126_206-A0-3_json_sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79a574",
   "metadata": {},
   "source": [
    "<a name='s3.2'></a>\n",
    "### Exploratory Data Analysis ###\n",
    "We can analyze and preprocess the data using the [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). We went ahead and converted the JSON files into a .csv text file for this purpose since it's otherwise a time consuming process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2153b-d6dc-4328-bbbd-b2ec1afd1232",
   "metadata": {},
   "source": [
    "Execute the below cells to analyze the data contained in the annotation file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Load the .csv into a DataFrame\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "annotated_frames=pd.read_csv('data/annotation.csv', converters={2:ast.literal_eval})\n",
    "print(\"Length of the full DF object:\", len(annotated_frames))\n",
    "annotated_frames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e875a",
   "metadata": {},
   "source": [
    "We can do a `DataFrame.groupby().size()` to see how many rows there are per `frame_no`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Check how many rows per frame_no\n",
    "annotated_frames.groupby('frame_no').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a6825",
   "metadata": {},
   "source": [
    "It appears every frame has the same number of objects - 130. Here we've identified an issue with our annotation in that the bounding boxes persist even after the car has left the frame until the end of the video. We will have to filter them out with the `outside` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Filter out annotations that do not have car inside the bbox\n",
    "filtered_frames=annotated_frames[annotated_frames[\"outside\"] == 0]\n",
    "print(\"Length of the filtered DF object:\", len(filtered_frames))\n",
    "filtered_frames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b4ab7-832e-4985-9417-b5e49db7fa92",
   "metadata": {},
   "source": [
    "The filtered DataFrame is much smaller. We can plot the _Frame Indices that Include Moving Cars_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Plot frames that include moving cars\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "frames_list=list(filtered_frames['frame_no'].unique())\n",
    "frame_existance=np.zeros(annotated_frames['frame_no'].max()+1)\n",
    "for i in frames_list:\n",
    "    frame_existance[int(i)]=1\n",
    "y_pos=np.arange(len(frame_existance))\n",
    "fig, ax=plt.subplots(figsize=(18, 3))\n",
    "plt.bar(y_pos, frame_existance, align='center', alpha=0.5)\n",
    "plt.title('Frame Indices that Include Moving Cars')\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef45127",
   "metadata": {},
   "source": [
    "The filtered annotated looks much more reasonable. It looks like there are many frames where there are no cars present in the frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e403c",
   "metadata": {},
   "source": [
    "<a name='s3.3'></a>\n",
    "### Convert Video File into Frame Images ###\n",
    "Because the object detection model operates on frame-based data, we will need to generate frames from the original movie file. To do so, we are going to use [OpenCV](https://opencv.org/) to open the video file and write a `.png` image file for each frame that has annotation. We will be using the original `.mp4` file at 10 FPS. In addition to converting video frames into `.png` images, we are creating a video for which the annotations are displayed as bounding boxes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03427fe3-bd0b-4c90-b1f1-6d38993a31db",
   "metadata": {},
   "source": [
    "Execute the below cells to create an annotated video and extract annotated images for the TAO Toolkit. This can take up to 5 mins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Define function to extract images and generate an annotated video\n",
    "import cv2\n",
    "colors = [(255, 255, 0), (255, 0, 255), (0, 255, 255), (0, 0, 255), (255, 0, 0), (0, 255, 0), (0, 0, 0), (255, 100, 0), (100, 255, 0), (100, 0, 255), (255, 0, 100)]\n",
    "\n",
    "def save_images(video_path, image_folder, frames_list, annotated_frames,  video_out_folder, fps=10):\n",
    "    # Create image folder if it doesn't exist\n",
    "    if not os.path.exists(image_folder):\n",
    "        print(\"Creating images folder\")\n",
    "        os.makedirs(image_folder)\n",
    "    \n",
    "    # Create directory for output video\n",
    "    if not os.path.exists(video_out_folder):\n",
    "        print(\"Creating video out folder\")\n",
    "        os.makedirs(video_out_folder)\n",
    "    \n",
    "    # Start reading input video\n",
    "    input_video=cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # cv2.VideoCapture().read() returns true if it has a next frame\n",
    "    retVal, im=input_video.read()\n",
    "    size=im.shape[1], im.shape[0]\n",
    "    fourcc=cv2.VideoWriter_fourcc('h','2','6','4') \n",
    "    \n",
    "    # Start writing output video\n",
    "    output_video=cv2.VideoWriter('{}/annotated_video.mp4'.format(video_out_folder), fourcc, fps, size)\n",
    "\n",
    "    frameCount=0\n",
    "    i=1\n",
    "    \n",
    "    # While has next frame\n",
    "    while retVal:\n",
    "        print(\"\\rProcessing frame no: {}\".format(frameCount), end='', flush=True)\n",
    "        \n",
    "        # If current frame is in the list of annotated frames, draw bounding box(es) and include in the output video\n",
    "        if frameCount in frames_list:\n",
    "            print(\"\\rSaving frame no: {}, index: {} out of {}\".format(frameCount, i, len(frames_list)), end='')\n",
    "            cv2.imwrite(os.path.join(image_folder, '{}.png'.format(frameCount)), im)\n",
    "            i+=1\n",
    "            frame_items=annotated_frames[annotated_frames[\"frame_no\"]==int(frameCount)]\n",
    "            for index, box in frame_items.iterrows():\n",
    "                xmin, ymin, xmax, ymax = box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]\n",
    "                xmin2, ymin2, xmax2, ymax2 = box[\"crop\"][0], box[\"crop\"][1], box[\"crop\"][2], box[\"crop\"][3]\n",
    "                cv2.rectangle(im, (xmin, ymin), (xmax, ymax), colors[0], 1)\n",
    "                cv2.rectangle(im, (int(xmin2), int(ymin2)), (int(xmax2), int(ymax2)), colors[1], 1)\n",
    "            output_video.write(im)\n",
    "\n",
    "        # Read next frame\n",
    "        retVal, im=input_video.read()\n",
    "        frameCount+=1\n",
    "\n",
    "    input_video.release()\n",
    "    output_video.release()\n",
    "    return size        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Extract images and generate an annotated video\n",
    "save_images('{}/126_206-A0-3_raw.mp4'.format(os.environ['SOURCE_DATA_DIR']), \n",
    "            '{}/{}'.format('{}/training'.format(os.environ['DATA_DIR']), 'images'),\n",
    "            frames_list,\n",
    "            filtered_frames,\n",
    "            '{}/{}'.format(os.environ['DATA_DIR'], 'video_out'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043185ff-923e-4fa4-ba55-346c2fe4ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View the annotated output video\n",
    "Video('tao_project/data/video_out/annotated_video.mp4', width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8dc9ad",
   "metadata": {},
   "source": [
    "<a name='s3.4'></a>\n",
    "### Generate Labels ###\n",
    "We also need to generate KITTI format labels for each frame, which is also described in the [TAO Toolkit User Guide](https://docs.nvidia.com/tao/tao-toolkit/text/data_annotation_format.html#label-files). A KITTI format label file is a simple text file containing one line per object. Each line has multiple fields. The sum of the total number of elements per object is 15 as shown below: <br>\n",
    "`class name`, `truncation`, `occlusion`, `alpha`, `xmin`, `ymin`, `xmax`, `ymax`, `height`, `weight`, `length`, `x`, `y`, `z`, `rotation_y` <br>\n",
    "Currently, for detection the TAO Toolkit only requires the class name and bbox coordinates fields to be populated. This is because the TAO Toolkit training pipe supports training only for class and bbox coordinates. The remaining fields may be set to 0 as placeholder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5df11-9c18-4a91-82a5-fa64685686a6",
   "metadata": {},
   "source": [
    "Execute the below cells to generate the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fee05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Generate labels in KITTI format\n",
    "label_folder='{}/training/labels'.format(os.environ['DATA_DIR'])\n",
    "if not os.path.exists(label_folder):\n",
    "    print(\"Creating labels folder\")\n",
    "    os.makedirs(label_folder)\n",
    "for frame in sorted(frames_list): \n",
    "    current_frame=filtered_frames[filtered_frames['frame_no']==frame]\n",
    "    with open('{}/{}.txt'.format(label_folder, frame), 'w') as f: \n",
    "        for i, box in current_frame.iterrows(): \n",
    "            print('Writing for frame {}'.format(frame), end='\\r')\n",
    "            f.write(\"Car 0 0 0 {} {} {} {} 0 0 0 0 0 0 0\\n\".format(box['xmin'], box['ymin'], box['xmax'], box['ymax']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANAGE THIS CELL\n",
    "# Preview sample KITTI format labels\n",
    "!cat $DATA_DIR/training/labels/20.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7df4f1",
   "metadata": {},
   "source": [
    "<a name='s3.5'></a>\n",
    "### Converting to TFRecords ###\n",
    "The TAO Toolkit enables converting the training data into the [**TFRecords**](https://www.tensorflow.org/tutorials/load_data/tfrecord) format, which is a simple format for storing a sequence of binary records. The TFRecord specification encodes an image frame and all the annotations associated with that frame into a single row. This can drastically help iterate faster through the data. The TAO Toolkit helps us easily convert data to TFRecord format once it's in the KITTI format. This can be done using the `dataset_convert` subtask. The `dataset_convert` tool requires a configuration file as input, which has the below parameters: \n",
    "* `kittie_config`\n",
    "    * `root_directory_path (str)`: Path to the data set root. \n",
    "    * `image_dir_name (str)`: Relative path to the directory containing images. \n",
    "    * `label_dir_name (str)`: Relative path to the directory containing labels. \n",
    "    * `partition_mode (str)`: Method _(\"random\" or \"sequence\")_ employed when partitioning the data into folds. \n",
    "    * `num_partitions (int)`: Number of partitions (folds) to split the data _(default=2)_. This field is ignored when the partition mode is set to \"random\" as by default only two partitions are generated: `train` and `val`. \n",
    "    * `image_extension (str)`: Extension of the images _(\".png\", \".jpg\", or \".jpeg\")_. \n",
    "    * `val_split (float)`: Percentage of data to be separated for validation _(0-100)_. \n",
    "    * `num_shards (int)`: The number of shards per fold _(1-20)_. When you have large amounts of samples, it is beneficial to shard your data set into multiple files as it allows inputs to 1) be read in parallel to improve throughput and 2) be shuffled better to improve performance of the model. This is particularly important when the data set is large. You can find more information on sharding on [TensorFlow's API documentation](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset#raises_1). \n",
    "* `image_directory_path (str)`: Path to the data set root. \n",
    "\n",
    "Once generated, you can use the TFRecords across multiple training experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5bcfc",
   "metadata": {},
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise #2 - Dataset Convert ####\n",
    "Let's use the `dataset_convert` subtask to generate TFRecords. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the [TFRecords conversion spec file](spec_files/kitti_config.txt) by changing the `<FIXME>`s into the correct values and **save changes**. \n",
    "* Execute the below cells to create TFRecords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e32483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View the spec file\n",
    "!cat $SPEC_FILES_DIR/kitti_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a7d55-f0ff-403d-923a-c1f877a8ba2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kitti_config {\n",
    "#   root_directory_path: \"/dli/task/tao_project/data/training\"\n",
    "#   image_dir_name: \"images\"\n",
    "#   label_dir_name: \"labels\"\n",
    "#   image_extension: \".png\"\n",
    "#   partition_mode: \"random\"\n",
    "#   num_partitions: 2\n",
    "#   val_split: 20\n",
    "#   num_shards: 10\n",
    "# }\n",
    "# image_directory_path: \"/dli/task/tao_project/data/training\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c1c8d-d612-40aa-9e48-843aae048fcf",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b268d7c-d554-4daf-8ce1-ee76da69e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View dataset_convert usage\n",
    "!detectnet_v2 dataset_convert --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477fce70-d634-48d6-94e1-72d5e366882a",
   "metadata": {},
   "source": [
    "When using the `dataset_convert` subtask, the `-o` argument indicates the output filename and the `-d` argument points to the path to the detection data set spec file containing the config for exporting `.tfrecord` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14481bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Create directory for TFRecords and delete existing files if they exist\n",
    "!mkdir -p $DATA_DIR/tfrecords && rm -rf $DATA_DIR/tfrecords/*\n",
    "\n",
    "!detectnet_v2 dataset_convert -d $SPEC_FILES_DIR/kitti_config.txt \\\n",
    "                              -o $DATA_DIR/tfrecords/kitti_trainval/kitti_trainval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd0daf",
   "metadata": {},
   "source": [
    "Check the shards that have been created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b2899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Check the shards that have been created\n",
    "!ls -rlt $DATA_DIR/tfrecords/kitti_trainval/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1d3a0",
   "metadata": {},
   "source": [
    "**Well Done**! When you're ready, let's move to the [next notebook](./03_model_training_with_the_TAO_Toolkit.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f613e",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
